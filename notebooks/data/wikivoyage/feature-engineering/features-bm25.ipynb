{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activities with BM25 - Old style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps taken previously:\n",
    "1. Select list of activities (We had 87 - from booking?)\n",
    "2. Train Word2Vec model on entire Wikivoyage corpus (no filtering!)\n",
    "3. For each activity: \n",
    "    - get vector with 50 most similar words\n",
    "    - manually remove words that are not relevant for a topic (output in [gsheet](https://docs.google.com/spreadsheets/d/1aucwUbyvVzBQ39lz4ipzKeBE30VADS_nFc2ey_iYo-8/edit#gid=0))\n",
    "    - what is left is the search query for the activity\n",
    "4. Get texts for all destinations in scope>\n",
    "5. Use BM25 to create a score for each place/activity pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '../../../../'\n",
    "feature_input_dir = 'src/stairway/wikivoyage/feature_engineering/features_input_data/'\n",
    "feature_terms_file = 'feature_terms.csv'\n",
    "feature_mapping_file = 'feature_profiles.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pd.read_csv(source_dir + feature_input_dir + feature_terms_file, header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = queries_df.apply(\n",
    "#     lambda x: ','.join(x.dropna().astype(str)),\n",
    "    lambda x: x.dropna().astype(str).tolist(),\n",
    "    axis=1\n",
    ")\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = pd.read_csv(source_dir + feature_input_dir + feature_mapping_file)\n",
    "types.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_wiki_in  = source_dir + 'data/wikivoyage/raw/enwikivoyage-20191001-pages-articles.xml.bz2'\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "wiki = WikiCorpus(path_wiki_in, article_min_tokens=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = list(wiki.get_texts())\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get index for places in scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(source_dir + 'data/wikivoyage/enriched/wikivoyage_destinations.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(source_dir + 'data/wikivoyage/clean/wikivoyage_metadata_all.csv')\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that df_all matches with corpus size! Otherwise indexing wouldn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(corpus) == len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get indices from df_all that are in scope\n",
    "scope = df_all.loc[lambda row: row['pageid'].isin(df['wiki_id'])][['pageid']]\n",
    "scope.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts for places in scope\n",
    "corpus_scope = [corpus[i] for i in scope.index]\n",
    "len(corpus_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25\n",
    "\n",
    "[Explaination of BM25](https://turi.com/learn/userguide/feature-engineering/bm25.html) including a Python example/libary. The transformed output is a column of type float with the BM25 score for each document.\n",
    "\n",
    "This implementation seems easiest to use: https://pypi.org/project/rank-bm25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25 = BM25Okapi(corpus_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = queries['art galleries']\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply bm25\n",
    "doc_scores = bm25.get_scores(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max scores and how many documents got a score bigger than 0\n",
    "print('min:', min(doc_scores), 'max:', max(doc_scores), '>0:', sum(doc_scores > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = np.argsort(doc_scores)[-5:]\n",
    "print(top_5)\n",
    "print(doc_scores[top_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[top_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to heavily bias towards places with relatively little text which contains a couple of the required terms. \n",
    "\n",
    "**TODO**: investigate how longer documents could still end up high in the ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = np.array([bm25.get_scores(queries[i]) for i in range(0, len(queries))]).T\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(scores, columns=queries.index)\n",
    "df_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add proper column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.columns = types['feature_name']\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine a top 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[np.argsort(df_scores['Whale watching'])[-5:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to old scores\n",
    "\n",
    "Note, the old scores had more places in scope so exact counts don't match. Also the BM25 implementation was done manually by Bram instead of importing a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_old = pd.read_csv(source_dir + \"data/old-sql-database/destination_scores.csv\")\n",
    "df_scores_old.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare distributions for some features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "df_scores['Museums'].hist(bins=30, ax=axes[0])\n",
    "axes[0].set_title('New scores. Count = {}'.format(sum(df_scores['Museums'] > 0)), size=15)\n",
    "\n",
    "df_scores_old['museums'].hist(bins=30, ax=axes[1]);\n",
    "axes[1].set_title('Old scores. Count = {}'.format(sum(df_scores_old['museums'] > 0)), size=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "df_scores['Islands'].hist(bins=30, ax=axes[0])\n",
    "axes[0].set_title('New scores. Count = {}'.format(sum(df_scores['Islands'] > 0)), size=15)\n",
    "\n",
    "df_scores_old['islands'].hist(bins=30, ax=axes[1]);\n",
    "axes[1].set_title('Old scores. Count = {}'.format(sum(df_scores_old['islands'] > 0)), size=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "# this time plot only > 0\n",
    "df_scores['Wineries'].loc[df_scores['Wineries'] > 0].hist(bins=30, ax=axes[0])\n",
    "axes[0].set_title('New scores. Count = {}'.format(sum(df_scores['Wineries'] > 0)), size=15)\n",
    "\n",
    "df_scores_old['wineries'].loc[df_scores_old['wineries'] > 0].hist(bins=30, ax=axes[1]);\n",
    "axes[1].set_title('Old scores. Count = {}'.format(sum(df_scores_old['wineries'] > 0)), size=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions are quite different. Possibly reasons:\n",
    "\n",
    "* New text data, things might have changed in wikivoyage\n",
    "* Different sizes and possibly different places in scope\n",
    "* Different implementations of BM25 (package vs. manual)\n",
    "* Different hyperparameters for BM25\n",
    "\n",
    "However counts are total counts per category and distributions are enough alike to accept the new scores as feature scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = source_dir + 'data/wikivoyage/enriched/wikivoyage_features.csv'\n",
    "\n",
    "df_final = pd.concat([df[['id']], df_scores], axis=1)\n",
    "df_final.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_path = 'api/data/wikivoyage_features.csv'\n",
    "\n",
    "df_final.to_csv(source_dir + api_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_path_types = 'api/data/wikivoyage_features_types.csv'\n",
    "\n",
    "types.to_csv(source_dir + api_path_types, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
