{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting place results\n",
    "\n",
    "Goal is to determine a nice sequence of place results for the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../../data/wikivoyage/'\n",
    "# folder where data should live for flask API\n",
    "api_dir = '../../../../api/data/'\n",
    "\n",
    "input_path = data_dir + 'processed/wikivoyage_destinations.csv'\n",
    "output_path1 = data_dir + 'enriched/wikivoyage_destinations.csv'\n",
    "output_path2 = api_dir + 'wikivoyage_destinations.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stairway.utils.utils import add_normalized_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nr_tokens'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['country', 'id', 'name']\n",
    "\n",
    "df[columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove destinations with no tokens\n",
    "\n",
    "Has to be done for resampling, otherwise there will be observations with weight 0 which means they will never get sampled and you can thus not 'sort' the *entire* data set as some observations aren't drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[lambda df: df['nr_tokens'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased sorting\n",
    "\n",
    "In order to get some randomness, but make sure the more important destinations get oversampled, use `nr_tokens` as a weight in the sampling method.\n",
    "\n",
    "For now, let's first have a look at the overall distribution of `nr_tokens` in our data. It is strongly skewed towards destinations with very few tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "#     .loc[lambda df: df['country'] == 'Netherlands']\n",
    "    .assign(nr_tokens_bins = lambda df: pd.cut(df['nr_tokens'], bins = list(range(0, 10000, 500)) + [99999]))\n",
    "    ['nr_tokens_bins']\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .plot(kind='bar')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that you don't want to random sample this way. It would mean that you would mostly show very unknown destinations to the user. \n",
    "\n",
    "Let's compare 3 different ways of sampling:\n",
    "1. without weights (so random)\n",
    "2. weighting by `nr_tokens`\n",
    "3. weighting by `nr_tokens` to the power `X`\n",
    "\n",
    "The more weighting, the more places are drawn with a larger number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_results = 16 # number of fetched results per API call\n",
    "power_factor = 1.5 # nr of times to the power of nr_tokens for sampling bigger documents\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=3, figsize=(16, 8*4))\n",
    "\n",
    "df_bins = (\n",
    "    df\n",
    "    .assign(nr_tokens_bins = lambda df: pd.cut(df['nr_tokens'], bins = list(range(0, 10000, 500)) + [99999]))\n",
    "    .assign(nr_tokens_powered = lambda df: df['nr_tokens']**power_factor)\n",
    ") \n",
    "\n",
    "for i, row in enumerate(axes):\n",
    "    for weights, ax in zip(['random', 'nr_tokens', 'nr_tokens^{}'.format(power_factor)], row):\n",
    "        \n",
    "        n = (i+1)*n_results\n",
    "        \n",
    "        # depending on weights type, sample differently\n",
    "        if weights == 'random':\n",
    "            df_plot = df_bins.sample(frac=1, random_state=1234)\n",
    "        elif weights == 'nr_tokens':\n",
    "            df_plot = df_bins.sample(frac=1, random_state=1234, weights='nr_tokens')\n",
    "        else: \n",
    "            df_plot = df_bins.sample(frac=1, random_state=1234, weights='nr_tokens_powered')\n",
    "        \n",
    "        # plot\n",
    "        (\n",
    "            df_plot\n",
    "            .head(n)\n",
    "            ['nr_tokens_bins']\n",
    "            .value_counts()\n",
    "            .sort_index()\n",
    "            .plot(kind='bar', ax=ax)\n",
    "        )\n",
    "        # prettify plot\n",
    "        if i < 7:\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "        ax.set_title('{} - {} obs'.format(weights, n))\n",
    "        \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power factor 1.5 seems to be nice. Powering even more will deplete the places with most observations very quickly. For the user this means that they first get all the well known destinations, and then the rest. The aim of our app is to surprise and inspire, so we also want to show more lesser known destinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV\n",
    "\n",
    "Add the sampling weight feature and write the final data set to be used by the frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stairway.wikivoyage.feature_engineering import add_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_factor = 1.5\n",
    "\n",
    "output_df = (\n",
    "    df\n",
    "    # add the feature\n",
    "    .pipe(add_sample_weight)\n",
    "    # other hygiene\n",
    "    .drop(columns=['nr_tokens', 'ispartof', 'parentid'])\n",
    "    .set_index('id', drop=False)\n",
    "    # need to do this to convert numpy int and float to native data types\n",
    "    .astype('object')\n",
    ")\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write 'approved' file to the data and api folders\n",
    "# output_df.to_csv(output_path1, index=False)\n",
    "# output_df.to_csv(output_path2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting based on profiles\n",
    "\n",
    "We want to allow the user to sort based on profiles like 'Nature', 'Culture', 'Beach'. To do this, we have identified which features are part of a profile. For the sorting, we then select the features in scope and sum their BM25 scores to get the final score for the sorting.\n",
    "\n",
    "The question is: do these BM25 scores bias towards smaller destinations? If yes, do we want to apply some kind of weighting with the number of tokens as is demonstrated above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'wikivoyage_destinations.csv'\n",
    "features_file_name = 'wikivoyage_features.csv'\n",
    "features_types = 'wikivoyage_features_types.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_places = pd.read_csv(data_dir + 'enriched/' + file_name).set_index(\"id\", drop=False)\n",
    "df_features = pd.read_csv(api_dir + features_file_name).set_index(\"id\")\n",
    "df_feature_types = pd.read_csv(api_dir + features_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.resources.utils.features import add_sorting_weight_by_profiles, sort_places_by_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = ['nature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_places_by_profiles(df_places, profiles, df_features, df_feature_types).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_results = 16 # number of fetched results per API call\n",
    "profiles = ['nature', 'city', 'culture', 'active', 'beach']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(16, 8*4))\n",
    "\n",
    "df_bins = (\n",
    "    df_places\n",
    "    .assign(nr_tokens_bins = lambda df: pd.cut(df['nr_tokens'], bins = list(range(0, 10000, 500)) + [99999]))\n",
    ") \n",
    "\n",
    "i = 0\n",
    "for profile, row in zip(profiles, axes):\n",
    "    i += 1\n",
    "    for j, ax in enumerate(row):\n",
    "        \n",
    "        n = (j+1)*n_results\n",
    "        \n",
    "        # depending on profile, sort differently\n",
    "        df_sorted = df_bins.pipe(sort_places_by_profiles, [profile], df_features, df_feature_types)\n",
    "        \n",
    "        # plot\n",
    "        (\n",
    "            df_sorted\n",
    "            .head(n)\n",
    "            ['nr_tokens_bins']\n",
    "            .value_counts()\n",
    "            .sort_index()\n",
    "            .plot(kind='bar', ax=ax)\n",
    "        )\n",
    "        # prettify plot\n",
    "        if i < len(profiles):\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "        ax.set_title('{} - {} obs'.format(profile, n))\n",
    "        \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our hypothesis that the sorting using BM25 weights heavily skews the top results towards destinations with little amount of tokens. So let's experiment a little, and scale the profile score with the number of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_results = 16 # number of fetched results per API call\n",
    "power_factor = 1.5\n",
    "profiles = ['nature', 'city', 'culture', 'active', 'beach']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(16, 8*4))\n",
    "\n",
    "df_bins = (\n",
    "    df_places\n",
    "    .assign(nr_tokens_bins = lambda df: pd.cut(df['nr_tokens'], bins = list(range(0, 10000, 500)) + [99999]))\n",
    "    .assign(nr_tokens_powered = lambda df: df['nr_tokens']**power_factor)\n",
    ") \n",
    "\n",
    "i = 0\n",
    "for profile, row in zip(profiles, axes):\n",
    "    i += 1\n",
    "    for j, ax in enumerate(row):\n",
    "        \n",
    "        n = (j+1)*n_results\n",
    "        \n",
    "        # depending on profile, sort differently\n",
    "        df_sorted = (\n",
    "            df_bins\n",
    "            .pipe(add_sorting_weight_by_profiles, [profile], df_features, df_feature_types)\n",
    "            .assign(weight = lambda df: df['nr_tokens'] * df['profile_weight'])\n",
    "            .sort_values('weight', ascending=False)\n",
    "        )\n",
    "        \n",
    "        # plot\n",
    "        (\n",
    "            df_sorted\n",
    "            .head(n)\n",
    "            ['nr_tokens_bins']\n",
    "            .value_counts()\n",
    "            .sort_index()\n",
    "            .plot(kind='bar', ax=ax)\n",
    "        )\n",
    "        # prettify plot\n",
    "        if i < len(profiles):\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "        ax.set_title('{} - {} obs'.format(profile, n))\n",
    "        \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That helps, although we seem to be overshooting a bit.... and now we are not even using the power factor. \n",
    "\n",
    "This could be because `nr_tokens` is of quite some magnitudes bigger than `profiles_weight`. Let's therefore try normalizing first and then adding both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_results = 16 # number of fetched results per API call\n",
    "power_factor = 1.5\n",
    "profiles = ['nature', 'city', 'culture', 'active', 'beach']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(16, 8*4))\n",
    "\n",
    "df_bins = (\n",
    "    df_places\n",
    "    .assign(nr_tokens_bins = lambda df: pd.cut(df['nr_tokens'], bins = list(range(0, 10000, 500)) + [99999]))\n",
    "    .assign(nr_tokens_norm = lambda df: (df['nr_tokens'] - df['nr_tokens'].min()) / (df['nr_tokens'].max() \n",
    "                                                                                     - df['nr_tokens'].min()))\n",
    ") \n",
    "\n",
    "i = 0\n",
    "for profile, row in zip(profiles, axes):\n",
    "    i += 1\n",
    "    for j, ax in enumerate(row):\n",
    "        \n",
    "        n = (j+1)*n_results\n",
    "        \n",
    "        # depending on profile, sort differently\n",
    "        df_sorted = (\n",
    "            df_bins\n",
    "            .pipe(add_sorting_weight_by_profiles, [profile], df_features, df_feature_types)\n",
    "            .assign(profile_weight_norm = lambda df: (df['profile_weight'] - df['profile_weight'].min()) / \n",
    "                    (df['profile_weight'].max() - df['profile_weight'].min()))\n",
    "            .assign(weight = lambda df: df['nr_tokens_norm'] + df['profile_weight_norm'])\n",
    "            .sort_values('weight', ascending=False)\n",
    "        )\n",
    "        \n",
    "        # plot\n",
    "        (\n",
    "            df_sorted\n",
    "            .head(n)\n",
    "            ['nr_tokens_bins']\n",
    "            .value_counts()\n",
    "            .sort_index()\n",
    "            .plot(kind='bar', ax=ax)\n",
    "        )\n",
    "        # prettify plot\n",
    "        if i < len(profiles):\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "        ax.set_title('{} - {} obs'.format(profile, n))\n",
    "        \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better :) \n",
    "\n",
    "Now give the profile scores an even higher weight by multiplicating that score before adding it. \n",
    "\n",
    "Tuning it a bit suggests that `multiplication_factor = 2` is too high, it favors more of the unknown destinations. But notably, even more important is that the right multiplication factor varies quite some per profile.. We will need to spend time to make something more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_results = 16 # number of fetched results per API call\n",
    "multiplication_factor = 1.5\n",
    "profiles = ['nature', 'city', 'culture', 'active', 'beach']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(16, 8*4))\n",
    "\n",
    "df_bins = (\n",
    "    df_places\n",
    "    .assign(nr_tokens_bins = lambda df: pd.cut(df['nr_tokens'], bins = list(range(0, 10000, 500)) + [99999]))\n",
    "    .pipe(add_normalized_column, 'nr_tokens')\n",
    ") \n",
    "\n",
    "i = 0\n",
    "for profile, row in zip(profiles, axes):\n",
    "    i += 1\n",
    "    for j, ax in enumerate(row):\n",
    "        \n",
    "        n = (j+1)*n_results\n",
    "        \n",
    "        # depending on profile, sort differently\n",
    "        df_sorted = (\n",
    "            df_bins\n",
    "            .pipe(add_sorting_weight_by_profiles, [profile], df_features, df_feature_types)\n",
    "            .pipe(add_normalized_column, 'profile_weight')\n",
    "            .assign(weight = lambda df: df['nr_tokens_norm'] + (df['profile_weight_norm']*multiplication_factor))\n",
    "            .sort_values('weight', ascending=False)\n",
    "        )\n",
    "        \n",
    "        # plot\n",
    "        (\n",
    "            df_sorted\n",
    "            .head(n)\n",
    "            ['nr_tokens_bins']\n",
    "            .value_counts()\n",
    "            .sort_index()\n",
    "            .plot(kind='bar', ax=ax)\n",
    "        )\n",
    "        # prettify plot\n",
    "        if i < len(profiles):\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "        ax.set_title('{} - {} obs'.format(profile, n))\n",
    "        \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just go for that for now, but add the requirement that there needs to be at least 0.1 feature_score (before normalisation), so that we know there won't be destinations added that have nothing to do with the profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
