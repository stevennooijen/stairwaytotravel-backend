{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikivoyage\n",
    "\n",
    "Latest version of the English source Wikivoyage content can be downloaded at:\n",
    "https://dumps.wikimedia.org/enwikivoyage/latest/\n",
    "\n",
    "Specific months can also be downloaded by adding yearmonth to the url:\n",
    "https://dumps.wikimedia.org/enwikivoyage/20191001/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data/wikivoyage/'\n",
    "\n",
    "path_wiki_in = data_dir + 'raw/enwikivoyage-20191001-pages-articles.xml.bz2'\n",
    "\n",
    "path_wiki_out = data_dir + 'processed/wikivoyage_dest_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements for base product\n",
    "\n",
    "structured:\n",
    "* destination name\n",
    "* parent (incl. hierarchy) -> country, continent\n",
    "* geolocation\n",
    "* (possibly: synonyms?)\n",
    "\n",
    "text:\n",
    "* activities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "\n",
    "Gensim has a `WikiCorpus` class that can be read to parse the wikitravel dump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "wiki = WikiCorpus(path_wiki_in, article_min_tokens=10) \n",
    "wiki.metadata = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wiki.metadata = True` adds `pageid` and `title` to each tokenized document.\n",
    "\n",
    "Some arguments to play with: \n",
    "- Only articles of sufficient length are returned (short articles & redirects etc are ignored). This is control by `article_min_tokens` on the class instance.\n",
    "- Set `token_min_len`, `token_max_len` as thresholds for token lengths that are returned (default to 2 and 15).\n",
    "\n",
    "Eventually, `wiki.get_texts()` can be used to retrieve the parsed contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (tokens, (pageid, title)) in islice(wiki.get_texts(), 5):\n",
    "    print(pageid, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how many documents were parsed in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages = [pageid for (tokens, (pageid, title)) in wiki.get_texts()]\n",
    "len(all_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Gensim to parse text content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning steps on text taken previously:\n",
    "1. lower case\n",
    "2. extracting type (city, park, region, country, continent)\n",
    "3. extracting status (outline, usable, guide, star)\n",
    "4. remove empty texts (maybe also throw away ones with very little text?)\n",
    "5. get geo coordinates\n",
    "6. get wikipedia link\n",
    "7. get parent\n",
    "8. get commons name (reference to other dataset)\n",
    "9. get DMOZ folder (reference to other dataset)\n",
    "10. add size of text\n",
    "11. set parents of continet to 'world'\n",
    "12. get parent ids by string matching ... follows from 'ispartof' ...\n",
    "13. throw away some specific stuff with parents like moon and space\n",
    "\n",
    "shit. that's a lot..!\n",
    "\n",
    "Instead of running the R scripts I built long time ago, it's probably better to adapt the Gensim code to parse this info on the fly. Let's make a copy of the Gensim code and create our own module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stairway.preprocessing import wikivoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with retrieving the following data from the text:\n",
    "\n",
    "```\n",
    "{{IsPartOf|North Brabant}}\n",
    "{{guidecity}}\n",
    "{{geo|51.69014|5.29897|zoom=15}}\n",
    "```\n",
    "\n",
    "logic of the class, happens in `get_texts()`:\n",
    "1. `extract_pages()` yields texts, pageid, and title. So this is for the **metadata**.\n",
    "2. `process_article()` in multithreated fashion. Converts texts into tokens. Need to adapt for parsing **text**\n",
    "    1. `filter_wiki()` filters out wiki markup from `raw`, leaving only text:\n",
    "        1. to unicode\n",
    "        2. decode html\n",
    "        3. `remove_markup()` filters out wiki markup from `text`, leaving only text.\n",
    "            1. `remove_template()` is finally the function that removes our fields of interest\n",
    "    2. `lemmatize()`. If wanted: lemmatizes text.\n",
    "    3. `tokenize()`.  Tokenizes text.\n",
    "   \n",
    "The `remove_markup()` function contains a lot of regex parsing. Let's adjust this function and see if instead of removing these regex strings, see if we can return it (together with the text). What it takes as an input is a text. So let's get one example text to work with first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_new = wikivoyage.WikiCorpus(path_wiki_in, article_min_tokens=10)\n",
    "wiki_new.metadata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = []\n",
    "for (pageid, title, patterns, text) in islice(wiki_new.get_texts(), 3):\n",
    "    example_texts.append(text)\n",
    "    \n",
    "example_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's further examine Gensim's logic by looking into the `remove_markup()` function.  It looks like the part that we look for is between `{{ ... }}` and is removed by the `remove_template()` function in it. Let's check what that does to our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import remove_markup, remove_template\n",
    "\n",
    "# remove_markup(example_texts[0], promote_remaining=True, simplify_links=True)\n",
    "# remove_template(example_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, remove template is doing this. So we need to alter something here. \n",
    "\n",
    "Also, one can look at the [template documentation](https://meta.wikimedia.org/wiki/Help:Template) to understand it a bit better.\n",
    "\n",
    "Now, let's try to adapt the function. Or instead of adapting it, let's add a function that retrieves the desired output and then apply the `remove_template()` after it to clean up as usual.\n",
    "\n",
    "First let's examine how the regex works in the Gensim code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_P3 = re.compile(r'{{([^}{]*)}}', re.DOTALL | re.UNICODE)\n",
    "re.search(RE_P3, example_texts[0]).groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out documentation on [regex syntax](https://docs.python.org/3/library/re.html?highlight=dotall#regular-expression-syntax) to break down this regex expression:\n",
    "- `[^}{]`:\n",
    "    - Special characters lose their special meaning inside sets. For example, [(+*)] will match any of the literal characters '(', '+', '*', or ')'.\n",
    "    - If the first character of the set is '^', all the characters that are not in the set will be matched.\n",
    "    - In normal words: match anything that is not a `{` or `}`\n",
    "- `*` Causes the resulting RE to match 0 or more repetitions of the preceding RE\n",
    "- `(...)` Matches whatever regular expression is inside the parentheses, and indicates the start and end of a group\n",
    "\n",
    "Great. So basically this defaults to 'match anything between `{{ ... }}`'.\n",
    "\n",
    "Now let's create our own pattern specific to one of our use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_P_Geo = re.compile(r'{{(geo|mapframe)\\|([-]?[0-9]+[.]?[0-9]*)\\|([-]?[0-9]+[.]?[0-9]*)([^}{]*)}}', \n",
    "                      re.DOTALL | re.UNICODE)\n",
    "\n",
    "match = re.search(RE_P_Geo, example_texts[0])\n",
    "match.groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now see if we can feed this back to the final output. First make a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns(s, pattern):\n",
    "\n",
    "    # get geo coordinates if available\n",
    "    match = re.search(pattern, s)\n",
    "    if match:\n",
    "        lat = match.group(2)\n",
    "        lon = match.group(3)\n",
    "        return lat, lon\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_patterns(example_texts[0], RE_P_Geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_patterns(example_texts[1], RE_P_Geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this, all we need to do is add this output to our own version of the WikiCorpus class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikivoyage.remove_markup(example_texts[0], extract_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, so now we just need to create similar functions for the other features of interest and pass the results on so that it all finally ends up in the output of the `WikiCorpus.get_texts()` function.\n",
    "\n",
    "Note: let's leave out links to Wikipedia, DMOZ and Commons databases for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (pageid, title, patterns, text) in islice(wiki_new.get_texts(), 5):\n",
    "    print(pageid, title, patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok bam! Let's get all data!\n",
    "\n",
    "#### Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_new.write_to_csv(path_wiki_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add number of tokens as feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_wiki_out)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: remove non-relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  ## CLEANING ON destination TITLES\n",
    "  dest_clean <- dest[!grepl('disambiguation', dest$title, ignore.case = TRUE), ] # clean disambiguation ('aberdeen')\n",
    "  dest_clean <- dest_clean[!grepl('wikivoyage', dest_clean$title, ignore.case = TRUE), ] # clean joke articles ('Mordor')\n",
    "  dest_clean <- dest_clean[!grepl('template', dest_clean$title, ignore.case = TRUE), ] # clean templates\n",
    "  dest_clean <- dest_clean[!grepl('mediawiki', dest_clean$title, ignore.case = TRUE), ] # clean MediaWiki\n",
    "  dest_clean <- dest_clean[!grepl('phrasebook', dest_clean$title, ignore.case = TRUE), ] # clean phrasebooks ('Ainu')\n",
    "  dest_clean <- dest_clean[!grepl('file:', dest_clean$title, ignore.case = TRUE), ] # clean files\n",
    "  dest_clean <- dest_clean[!grepl('category:', dest_clean$title, ignore.case = TRUE), ] # clean categories.\n",
    "  dest <- dest_clean\n",
    "  rm(dest_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check out find interlinks\n",
    "\n",
    "is a function in gensim wikicorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Capturing metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "import csv\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_dest = data_dir + 'processed/dest_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE difference in writing csv between python 2 and 3!\n",
    "# open file with encoding that can handle all characters!\n",
    "f = open(file_name_dest, \"w\", newline=\"\", encoding='utf-8') \n",
    "writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONNUMERIC)\n",
    "#writer.writerow( ('page_nr', 'title', 'id', 'redirect_title') )\n",
    "writer.writerow( ('title', 'id', 'redirect_title') )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namespace is considered to be unstable according to the [Gensim documentation](https://github.com/RaRe-Technologies/gensim/blob/3e027c252eac3cf7e613f425ad8b070e8fe88065/gensim/corpora/wikicorpus.py#L411). Follow their code for a more flexible approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## capture redirects too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#count = 0\n",
    "ns = '{http://www.mediawiki.org/xml/export-0.10/}' # set the namespace of the XML document\n",
    "# events = 'starts' makes sure that everytime a node opens, iterparse starts processing\n",
    "for (event, node) in etree.iterparse(wiki_xml, events=['end']):\n",
    "    if node.tag == ns+'page': # only parse page nodes, add namespace before because of long string otherwise\n",
    "        #count = count+1\n",
    "        title = node.find(ns+'title').text # find title node and retrieve its contents\n",
    "        wikiv_id = int(node.find(ns+'id').text) # find id node and convert contents to numeric\n",
    "        if node.find(ns+'redirect') is None : # check if redirect exists, if not replace with 'NA'\n",
    "            redirect = None\n",
    "        else: \n",
    "            redirect = node.find(ns+'redirect').attrib.get('title')\n",
    "        writer.writerow( (title, wikiv_id, redirect ) ) # write destination info \n",
    "        #writer.writerow( (count, title, wikiv_id, redirect ) ) # write destination info \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# libraries used\n",
    "import sys\n",
    "import xml.etree.ElementTree as etree\n",
    "import csv\n",
    "\n",
    "# Get the arguments passed in\n",
    "wikidump_name = sys.argv[1]\n",
    "file_name_dest = sys.argv[2]\n",
    "file_name_corp = sys.argv[3]\n",
    "\n",
    "\n",
    "#####------------------------       Code Body      ------------------------#####\n",
    "\n",
    "\n",
    "## parsing destination information \n",
    "\n",
    "# NOTE difference in writing csv between python 2 and 3!\n",
    "# open file with encoding that can handle all characters!\n",
    "f = open(file_name_dest, \"w\", newline=\"\", encoding='utf-8') \n",
    "writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONNUMERIC)\n",
    "#writer.writerow( ('page_nr', 'title', 'id', 'redirect_title') )\n",
    "writer.writerow( ('title', 'id', 'redirect_title') )\n",
    "\n",
    "#count = 0\n",
    "ns = '{http://www.mediawiki.org/xml/export-0.10/}' # set the namespace of the XML document\n",
    "# events = 'starts' makes sure that everytime a node opens, iterparse starts processing\n",
    "for (event, node) in etree.iterparse(wikidump_name, events=['end']):\n",
    "    if node.tag == ns+'page': # only parse page nodes, add namespace before because of long string otherwise\n",
    "        #count = count+1\n",
    "        title = node.find(ns+'title').text # find title node and retrieve its contents\n",
    "        wikiv_id = int(node.find(ns+'id').text) # find id node and convert contents to numeric\n",
    "        if node.find(ns+'redirect') is None : # check if redirect exists, if not replace with 'NA'\n",
    "            redirect = None\n",
    "        else: \n",
    "            redirect = node.find(ns+'redirect').attrib.get('title')\n",
    "        writer.writerow( (title, wikiv_id, redirect ) ) # write destination info \n",
    "        #writer.writerow( (count, title, wikiv_id, redirect ) ) # write destination info \n",
    "f.close()\n",
    "\n",
    "## parsing text information\n",
    "\n",
    "# NOTE difference in writing csv between python 2 and 3!\n",
    "# open file with encoding that can handle all characters!\n",
    "f = open(file_name_corp, \"w\", newline=\"\", encoding='utf-8') \n",
    "writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_NONNUMERIC)\n",
    "writer.writerow( ('id', 'text') )\n",
    "\n",
    "ns = '{http://www.mediawiki.org/xml/export-0.10/}' # set the namespace of the XML document\n",
    "# events = 'starts' makes sure that everytime a node opens, iterparse starts processing\n",
    "for (event, node) in etree.iterparse(wikidump_name, events=['end']):\n",
    "    if node.tag == ns+'page': # only parse page nodes, add namespace before because of long string otherwise\n",
    "        #title = node.find(ns+'title').text # find title node and retrieve its contents\n",
    "        wikiv_id = int(node.find(ns+'id').text) # find id node and convert contents to numeric\n",
    "        text = node.find(ns+'revision').find(ns+'text').text # find text node within the revision node \n",
    "        writer.writerow( (wikiv_id, text) ) # write destination info \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Capturing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suggest to use Gensim. have a lot of text parsing in it. or straight to vectors depending on what you want to do.\n",
    "\n",
    "For activity types. straight to vectors might just be perfect.\n",
    "\n",
    "Cleaning steps on text taken previously:\n",
    "1. lower case\n",
    "2. extracting type (city, park, region, country, continent)\n",
    "3. extracting status (outline, usable, guide, star)\n",
    "4. remove empty texts (maybe also throw away ones with very little text?)\n",
    "5. get geo coordinates\n",
    "6. get wikipedia link\n",
    "7. get parent\n",
    "8. get commons name (reference to other dataset)\n",
    "9. get DMOZ folder (reference to other dataset)\n",
    "10. add size of text\n",
    "11. set parents of continet to 'world'\n",
    "12. get parent ids by string matching ... follows from 'ispartof' ...\n",
    "13. throw away some specific stuff with parents like moon and space\n",
    "\n",
    "shit. that's a lot..!\n",
    "\n",
    "Probably better to run the default scripts I have again, than trying to adapt the Gensim functions to parse this info. Then just run Gensim to get the cleaned text. >> but Gensim code is well organized. Could just go and adjust the `remove_markup()` function. This cleans a lot. But instead of cleaning we could get the info we need. Maybe try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and parsing raw wikivoyage corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Spacy conference I tried to label activities in the wikivoyage corpus\n",
    "\n",
    "The way of working I documented in OneNote. There is also a video\n",
    "tutorial online that describes the steps pretty well.\n",
    "\n",
    "Some notes on what I did that training:\n",
    "\n",
    "Parse the latest version of the wikivoyage dataset with Bram's: code\n",
    "```\n",
    "python process_wiki.py enwikivoyage-latest-pages-articles.xml.bz2 data/wiki.processed.csv\n",
    "```\n",
    "\n",
    "However, for the labelling in Prodigy this is not good enough. Actually\n",
    "we need the data on a per sentence level, because labelling entire\n",
    "texts of a destination is too much text. Therefore the next step would\n",
    "be to adjust Bram's parser so that it doesn't remove punctuation.\n",
    "\n",
    "The to be flow would then be something like:\n",
    "* Parse wikipedia corpus in `.bz2` format, output `.jsonl` per sentence\n",
    "* Add metadata on the source page to the parsed lines in `.jsonl`\n",
    "* `.jsonl` necessary for classification per sentence.\n",
    "\n",
    "example format:\n",
    "```\n",
    "{\"text\":\"Uber\\u2019s Lesson: Silicon Valley\\u2019s Start-Up Machine Needs Fixing\",\"meta\":{\"source\":\"The New York Times\"}}\n",
    "{\"text\":\"Pearl Automation, Founded by Apple Veterans, Shuts Down\",\"meta\":{\"source\":\"The New York Times\"}}\n",
    "```\n",
    "\n",
    "Then use `textcat.teach` with `source` argument\n",
    "\n",
    "possibly build own custom corpus loader for wiki:\n",
    "https://support.prodi.gy/t/template-for-prodigy-corpus-and-api-loaders/331/4\n",
    "\n",
    "For the labelling we have to choose ourselves which LABELS to use.\n",
    "\n",
    "The corpus to start training on would best be `en_vectors_web_lg` as\n",
    "this has the best text representation (vectors), without having the NER\n",
    "and dependency crap.\n",
    "\n",
    "When we tried Prodigy with the entire destination texts per time we\n",
    "noted that `textcat.teach` is going through the texts alphabetically,\n",
    "you might want to change this too such that it selects the sentences\n",
    "it is most uncertain about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_in = '../../data/wikivoyage/raw/enwikivoyage-latest-pages-articles.xml.bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If pattern package is installed, use fancier shallow parsing to get token lemmas. Otherwise, use simple regexp tokenization. You can override this automatic logic by forcing the lemmatize parameter explicitly. self.metadata if set to true will ensure that serialize will write out article titles to a pickle file.\n",
    "\n",
    "https://www.pydoc.io/pypi/gensim-3.2.0/autoapi/corpora/wikicorpus/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import logging\n",
    "# import os.path\n",
    "# import sys\n",
    "# import csv\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "\n",
    "wiki = WikiCorpus(wiki_in, lemmatize=False)\n",
    "wiki.metadata = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wiki.get_texts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in islice(wiki.get_texts(), 1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need a custom tokenizer as currently all punctuation is removed and thus you cannot label per sentence..\n",
    "\n",
    "Current parser also doesn't yield important metadata like the parent, and removes all structure from the file (i.e. on the sections). \n",
    "\n",
    "also structure terms like 'buy' need to be removed. see screenshot of tagger\n",
    "\n",
    "code from `corpora.wikicorpus` should be only. see if we can adjust the `process_article` or `tokenizer_func` methods.\n",
    "\n",
    "https://www.pydoc.io/pypi/gensim-3.2.0/autoapi/corpora/wikicorpus/index.html#module-corpora.wikicorpus\n",
    "\n",
    "source code: https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py\n",
    "* seems like we can replace the `tokenizer` to prevent the `.` to be removed: https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do something here to write to jsonl?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (tokens, (pageid, title)) in wiki.get_texts():\n",
    "    if type(tokens[0]) == str:\n",
    "        row = [title, ' '.join(tokens)]\n",
    "    else:\n",
    "        row = [title, b' '.join(tokens).decode('utf-8')]\n",
    "    csv_out.writerow(row)\n",
    "    i = i + 1\n",
    "    if (i % 10000 == 0):\n",
    "        logger.info(\"Saved \" + str(i) + \" articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just import after brams run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/wikivoyage/processed/wiki-processed.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Wrap pattern column in a dictionary\n",
    "df[\"json\"] = df.apply(lambda x: {\"text\": x[1], \"meta\" : {\"place\": x[0]}}, axis=1)\n",
    "\n",
    "# Output in JSONL format\n",
    "df['json'].to_json('../../data/wikivoyage/processed/wiki-processed-prodigy.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to jsonl\n",
    "\n",
    "possibly use prodigy's build in functionality: https://support.prodi.gy/t/jsonl-format/783/2\n",
    "\n",
    "or write to `jsonl` from pandas: https://stackoverflow.com/questions/51775175/pandas-dataframe-to-jsonl-json-lines-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "from pathlib import Path\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"Read a .jsonl file and yield its contents line by line.\n",
    "    file_path (unicode / Path): The file path.\n",
    "    YIELDS: The loaded JSON contents of each line.\n",
    "    \"\"\"\n",
    "    with Path(file_path).open('r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            try:  # hack to handle broken jsonl\n",
    "                yield ujson.loads(line.strip())\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "\n",
    "def write_jsonl(file_path, lines):\n",
    "    \"\"\"Create a .jsonl file and dump contents.\n",
    "    file_path (unicode / Path): The path to the output file.\n",
    "    lines (list): The JSON-serializable contents of each line.\n",
    "    \"\"\"\n",
    "    data = [ujson.dumps(line, escape_forward_slashes=False) for line in lines]\n",
    "    Path(file_path).open('w', encoding='utf-8').write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl('test.jsonl', [\"abra\", \"cadabra\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(outp, 'w', encoding='utf-8') as output:\n",
    "    csv_out = csv.writer(output, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    for (tokens, (pageid, title)) in wiki.get_texts():\n",
    "        # print (tokens)\n",
    "        # output.write(b' '.join(title).decode('utf-8') + '\\n')\n",
    "        # output.write(\"\\\"\" + title + '\\\",\\\"' + b' '.join(tokens).decode('utf-8') + '\\\"\\n')\n",
    "        # print(type(b' '.join(tokens).decode('utf-8')))\n",
    "        # row = [title, b' '.join(tokens).decode('utf-8')]\n",
    "        if type(tokens[0]) == str:\n",
    "            row = [title, ' '.join(tokens)]\n",
    "        else:\n",
    "            row = [title, b' '.join(tokens).decode('utf-8')]\n",
    "        csv_out.writerow(row)\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            logger.info(\"Saved \" + str(i) + \" articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     program = os.path.basename(sys.argv[0])\n",
    "#     logger = logging.getLogger(program)\n",
    "\n",
    "#     logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "#     logging.root.setLevel(level=logging.INFO)\n",
    "#     logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "    # check and process input arguments\n",
    "#     if len(sys.argv) < 3:\n",
    "#         print(globals()['__doc__'] % locals())\n",
    "#         sys.exit(1)\n",
    "#     inp, outp = sys.argv[1:3]\n",
    "    space = \" \"\n",
    "    i = 0\n",
    "    # output = open(outp, 'w')\n",
    "    wiki = WikiCorpus(inp, lemmatize=False, dictionary={})\n",
    "    wiki.metadata = True\n",
    "\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "\n",
    "    with open(outp, 'w', encoding='utf-8') as output:\n",
    "        csv_out = csv.writer(output, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "        for (tokens, (pageid, title)) in wiki.get_texts():\n",
    "            # print (tokens)\n",
    "            # output.write(b' '.join(title).decode('utf-8') + '\\n')\n",
    "            # output.write(\"\\\"\" + title + '\\\",\\\"' + b' '.join(tokens).decode('utf-8') + '\\\"\\n')\n",
    "            # print(type(b' '.join(tokens).decode('utf-8')))\n",
    "            # row = [title, b' '.join(tokens).decode('utf-8')]\n",
    "            if type(tokens[0]) == str:\n",
    "                row = [title, ' '.join(tokens)]\n",
    "            else:\n",
    "                row = [title, b' '.join(tokens).decode('utf-8')]\n",
    "            csv_out.writerow(row)\n",
    "            i = i + 1\n",
    "            if (i % 10000 == 0):\n",
    "                logger.info(\"Saved \" + str(i) + \" articles\")\n",
    "\n",
    "    output.close()\n",
    "    logger.info(\"Finished Saved \" + str(i) + \" articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "    # check and process input arguments\n",
    "    if len(sys.argv) < 3:\n",
    "        print(globals()['__doc__'] % locals())\n",
    "        sys.exit(1)\n",
    "    inp, outp = sys.argv[1:3]\n",
    "    space = \" \"\n",
    "    i = 0\n",
    "    # output = open(outp, 'w')\n",
    "    wiki = WikiCorpus(inp, lemmatize=False, dictionary={})\n",
    "    wiki.metadata = True\n",
    "\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "\n",
    "    with open(outp, 'w', encoding='utf-8') as output:\n",
    "        csv_out = csv.writer(output, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "        for (tokens, (pageid, title)) in wiki.get_texts():\n",
    "            # print (tokens)\n",
    "            # output.write(b' '.join(title).decode('utf-8') + '\\n')\n",
    "            # output.write(\"\\\"\" + title + '\\\",\\\"' + b' '.join(tokens).decode('utf-8') + '\\\"\\n')\n",
    "            # print(type(b' '.join(tokens).decode('utf-8')))\n",
    "            # row = [title, b' '.join(tokens).decode('utf-8')]\n",
    "            if type(tokens[0]) == str:\n",
    "                row = [title, ' '.join(tokens)]\n",
    "            else:\n",
    "                row = [title, b' '.join(tokens).decode('utf-8')]\n",
    "            csv_out.writerow(row)\n",
    "            i = i + 1\n",
    "            if (i % 10000 == 0):\n",
    "                logger.info(\"Saved \" + str(i) + \" articles\")\n",
    "\n",
    "    output.close()\n",
    "    logger.info(\"Finished Saved \" + str(i) + \" articles\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
