{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikivoyage\n",
    "\n",
    "Latest version of the English source Wikivoyage content can be downloaded at:\n",
    "https://dumps.wikimedia.org/enwikivoyage/latest/\n",
    "\n",
    "Specific months can also be downloaded by adding yearmonth to the url:\n",
    "https://dumps.wikimedia.org/enwikivoyage/20191001/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data/wikivoyage/'\n",
    "\n",
    "path_wiki_in  = data_dir + 'raw/enwikivoyage-20191001-pages-articles.xml.bz2'\n",
    "path_wiki_out = data_dir + 'clean/wikivoyage_metadata_all.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements for base product\n",
    "\n",
    "structured:\n",
    "* destination name\n",
    "* parent (incl. hierarchy) -> country, continent\n",
    "* geolocation\n",
    "* (possibly: synonyms?)\n",
    "\n",
    "text:\n",
    "* activities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "\n",
    "Gensim has a `WikiCorpus` class that can be read to parse the wikitravel dump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "wiki = WikiCorpus(path_wiki_in, article_min_tokens=10) \n",
    "wiki.metadata = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wiki.metadata = True` adds `pageid` and `title` to each tokenized document.\n",
    "\n",
    "Some arguments to play with: \n",
    "- Only articles of sufficient length are returned (short articles & redirects etc are ignored). This is control by `article_min_tokens` on the class instance.\n",
    "- Set `token_min_len`, `token_max_len` as thresholds for token lengths that are returned (default to 2 and 15).\n",
    "\n",
    "Eventually, `wiki.get_texts()` can be used to retrieve the parsed contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (tokens, (pageid, title)) in islice(wiki.get_texts(), 5):\n",
    "    print(pageid, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how many documents were parsed in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages = [pageid for (tokens, (pageid, title)) in wiki.get_texts()]\n",
    "len(all_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Gensim to parse text content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning steps on text taken previously:\n",
    "1. lower case\n",
    "2. extracting type (city, park, region, country, continent)\n",
    "3. extracting status (outline, usable, guide, star)\n",
    "4. remove empty texts (maybe also throw away ones with very little text?)\n",
    "5. get geo coordinates\n",
    "6. get wikipedia link\n",
    "7. get parent\n",
    "8. get commons name (reference to other dataset)\n",
    "9. get DMOZ folder (reference to other dataset)\n",
    "10. add size of text\n",
    "11. set parents of continet to 'world'\n",
    "12. get parent ids by string matching ... follows from 'ispartof' ...\n",
    "13. throw away some specific stuff with parents like moon and space\n",
    "\n",
    "shit. that's a lot..!\n",
    "\n",
    "Instead of running the R scripts I built long time ago, it's probably better to adapt the Gensim code to parse this info on the fly. Let's make a copy of the Gensim code and create our own module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stairway.wikivoyage import parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with retrieving the following data from the text:\n",
    "\n",
    "```\n",
    "{{IsPartOf|North Brabant}}\n",
    "{{guidecity}}\n",
    "{{geo|51.69014|5.29897|zoom=15}}\n",
    "```\n",
    "\n",
    "logic of the class, happens in `get_texts()`:\n",
    "1. `extract_pages()` yields texts, pageid, and title. So this is for the **metadata**.\n",
    "2. `process_article()` in multithreated fashion. Converts texts into tokens. Need to adapt for parsing **text**\n",
    "    1. `filter_wiki()` filters out wiki markup from `raw`, leaving only text:\n",
    "        1. to unicode\n",
    "        2. decode html\n",
    "        3. `remove_markup()` filters out wiki markup from `text`, leaving only text.\n",
    "            1. `remove_template()` is finally the function that removes our fields of interest\n",
    "    2. `lemmatize()`. If wanted: lemmatizes text.\n",
    "    3. `tokenize()`.  Tokenizes text.\n",
    "   \n",
    "The `remove_markup()` function contains a lot of regex parsing. Let's adjust this function and see if instead of removing these regex strings, see if we can return it (together with the text). What it takes as an input is a text. So let's get one example text to work with first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_new = wikivoyage.WikiCorpus(path_wiki_in, article_min_tokens=10)\n",
    "wiki_new.metadata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = []\n",
    "for (pageid, title, redirect, nr_tokens, patterns, text) in islice(wiki_new.get_texts(), 10):\n",
    "    example_texts.append(text)\n",
    "    \n",
    "example_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's further examine Gensim's logic by looking into the `remove_markup()` function.  It looks like the part that we look for is between `{{ ... }}` and is removed by the `remove_template()` function in it. Let's check what that does to our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import remove_markup, remove_template\n",
    "\n",
    "# remove_markup(example_texts[0], promote_remaining=True, simplify_links=True)\n",
    "# remove_template(example_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, remove template is doing this. So we need to alter something here. \n",
    "\n",
    "Also, one can look at the [template documentation](https://meta.wikimedia.org/wiki/Help:Template) to understand it a bit better.\n",
    "\n",
    "Now, let's try to adapt the function. Or instead of adapting it, let's add a function that retrieves the desired output and then apply the `remove_template()` after it to clean up as usual.\n",
    "\n",
    "First let's examine how the regex works in the Gensim code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_P3 = re.compile(r'{{([^}{]*)}}', re.DOTALL | re.UNICODE)\n",
    "re.search(RE_P3, example_texts[0]).groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out documentation on [regex syntax](https://docs.python.org/3/library/re.html?highlight=dotall#regular-expression-syntax) to break down this regex expression:\n",
    "- `[^}{]`:\n",
    "    - Special characters lose their special meaning inside sets. For example, [(+*)] will match any of the literal characters '(', '+', '*', or ')'.\n",
    "    - If the first character of the set is '^', all the characters that are not in the set will be matched.\n",
    "    - In normal words: match anything that is not a `{` or `}`\n",
    "- `*` Causes the resulting RE to match 0 or more repetitions of the preceding RE\n",
    "- `(...)` Matches whatever regular expression is inside the parentheses, and indicates the start and end of a group\n",
    "\n",
    "Great. So basically this defaults to 'match anything between `{{ ... }}`'.\n",
    "\n",
    "Now let's create our own pattern specific to one of our use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_P_Geo = re.compile(r'{{(geo|mapframe)\\|([-]?[0-9]+[.]?[0-9]*)\\|([-]?[0-9]+[.]?[0-9]*)([^}{]*)}}', \n",
    "                      re.DOTALL | re.UNICODE)\n",
    "\n",
    "match = re.search(RE_P_Geo, example_texts[0])\n",
    "match.groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now see if we can feed this back to the final output. First make a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns(s, pattern):\n",
    "\n",
    "    # get geo coordinates if available\n",
    "    match = re.search(pattern, s)\n",
    "    if match:\n",
    "        lat = match.group(2)\n",
    "        lon = match.group(3)\n",
    "        return lat, lon\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_patterns(example_texts[0], RE_P_Geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_patterns(example_texts[1], RE_P_Geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we have to find the last occurance of a match. For example a page can have multiple `IsPartOf`s as is the case for \"Azores\":\n",
    "\n",
    "```\n",
    "{{isPartOf|Islands of the Atlantic Ocean}}\n",
    "{{isPartOf|Portugal}}\n",
    "```\n",
    "\n",
    "Or more often, there are multiple GEO tags. \n",
    "\n",
    "We resolve this by adding `(?s:.*)` in front of the regex which will make sure it will match the furthest and gradually backs of. However, this does slow down the speed of parsing the wikivoyage data considerably. So do remember to fix this if you want to speed up the code.\n",
    "\n",
    "Other tweeks that have been done is cleaning the strings of parsed text. For example we trim whitespace and we need to replace `_` in `ispartof` titles with spaces to avoid mismatches like:\n",
    "* `ispartof`: Lhasa_(prefecture) vs. `title`: Lhasa (prefecture)\t\n",
    "* `ispartof`: West_Yorkshire vs. `title`: West Yorkshire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this, all we need to do is add this output to our own version of the WikiCorpus class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikivoyage.remove_markup(example_texts[0], extract_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, so now we just need to create similar functions for the other features of interest and pass the results on so that it all finally ends up in the output of the `WikiCorpus.get_texts()` function.\n",
    "\n",
    "Note: let's leave out links to Wikipedia, DMOZ and Commons databases for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (pageid, title, redirect, nr_tokens, patterns, text) in islice(wiki_new.get_texts(), 10):\n",
    "    print(pageid, title, redirect, nr_tokens, patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok bam! Let's get all data!\n",
    "\n",
    "#### Write to CSV\n",
    "\n",
    "Write it to a CSV so it can be preprocessed further in another piece of code. Set `article_min_tokens=0` to get all redirects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_new = wikivoyage.WikiCorpus(path_wiki_in, article_min_tokens=0)\n",
    "wiki_new.metadata = True\n",
    "\n",
    "wiki_new.write_to_csv(path_wiki_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all texts\n",
    "\n",
    "Can be usefull for debugging purposes, where you want to look up specific destinations and what is happening there in the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "for (pageid, title, nr_tokens, patterns, text) in wiki_new.get_texts():\n",
    "    all_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_texts[1130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse text into tokens\n",
    "\n",
    "To just parse the text and not return metadata set `metadata` to False (the default option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_new = wikivoyage.WikiCorpus(path_wiki_in, article_min_tokens=10)\n",
    "wiki_new.metadata = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tokens = []\n",
    "\n",
    "for tokens in islice(wiki_new.get_texts(), 2):\n",
    "    example_tokens.append(tokens)\n",
    "    \n",
    "example_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing redirects\n",
    "\n",
    "We need to capture redirects to make sure we can complete the full hierarchy for each destination. \n",
    "\n",
    "For example:\n",
    "* \"Madeira\" and \"Saint Helena, Ascension and Tristan da Cunha\" have `{{IsPartOf|Islands of the Atlantic Ocean}}`\n",
    "* However, \"Islands of the Atlantic Ocean\" in itself is a redirect to \"South Atlantic Islands\":\n",
    "```xml\n",
    "    <title>South Atlantic islands</title>\n",
    "    <ns>0</ns>\n",
    "    <id>33370</id>\n",
    "    <redirect title=\"Islands of the Atlantic Ocean\" />\n",
    "```\n",
    "* This means we cannot traverse the tree further if we wouldn't have this redirect information.\n",
    "\n",
    "To capture the redirect information the following line of code was added to our own `Wikivoyage` Class:\n",
    "\n",
    "```python\n",
    "redirect = node.find(ns+'redirect').attrib.get('title')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: check out find interlinks\n",
    "\n",
    "is a function in gensim wikicorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
