{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting place results\n",
    "\n",
    "Goal is to determine a nice sequence of place results for the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/wikivoyage/'\n",
    "# folder where data should live for flask API\n",
    "api_dir = '../../api/data/'\n",
    "\n",
    "input_path = data_dir + 'processed/wikivoyage_destinations.csv'\n",
    "output_path1 = data_dir + 'enriched/wikivoyage_destinations.csv'\n",
    "output_path2 = api_dir + 'wikivoyage_destinations.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove destinations with no tokens\n",
    "\n",
    "Has to be done for resampling, otherwise there will be observations with weight 0 which means they will never get sampled and you can thus not 'sort' the *entire* data set as some observations aren't drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[lambda df: df['nr_tokens'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased sorting\n",
    "\n",
    "In order to get some randomness, but make sure the more important destinations get oversampled, use `nr_tokens` as a weight in the sampling method.\n",
    "\n",
    "For now, let's first have a look at the overall distribution of `nr_tokens` in our data. It is strongly skewed towards destinations with very few tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "#     .loc[lambda df: df['country'] == 'Netherlands']\n",
    "    .assign(nr_tokens_bins = lambda df: pd.cut(df['nr_tokens'], bins = list(range(0, 10000, 500)) + [99999]))\n",
    "    ['nr_tokens_bins']\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .plot(kind='bar')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that you don't want to random sample this way. It would mean that you would mostly show very unknown destinations to the user. \n",
    "\n",
    "Let's compare 3 different ways of sampling:\n",
    "1. without weights (so random)\n",
    "2. weighting by `nr_tokens`\n",
    "3. weighting by `nr_tokens` to the power `X`\n",
    "\n",
    "The more weighting, the more places are drawn with a larger number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_results = 16 # number of fetched results per API call\n",
    "power_factor = 1.5 # nr of times to the power of nr_tokens for sampling bigger documents\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=3, figsize=(16, 8*4))\n",
    "\n",
    "df_bins = (\n",
    "    df\n",
    "    .assign(nr_tokens_bins = lambda df: pd.cut(df['nr_tokens'], bins = list(range(0, 10000, 500)) + [99999]))\n",
    "    .assign(nr_tokens_powered = lambda df: df['nr_tokens']**power_factor)\n",
    ") \n",
    "\n",
    "for i, row in enumerate(axes):\n",
    "    for weights, ax in zip(['random', 'nr_tokens', 'nr_tokens^{}'.format(power_factor)], row):\n",
    "        \n",
    "        n = (i+1)*n_results\n",
    "        \n",
    "        # depending on weights type, sample differently\n",
    "        if weights == 'random':\n",
    "            df_plot = df_bins.sample(frac=1, random_state=1234)\n",
    "        elif weights == 'nr_tokens':\n",
    "            df_plot = df_bins.sample(frac=1, random_state=1234, weights='nr_tokens')\n",
    "        else: \n",
    "            df_plot = df_bins.sample(frac=1, random_state=1234, weights='nr_tokens_powered')\n",
    "        \n",
    "        # plot\n",
    "        (\n",
    "            df_plot\n",
    "            .head(n)\n",
    "            ['nr_tokens_bins']\n",
    "            .value_counts()\n",
    "            .sort_index()\n",
    "            .plot(kind='bar', ax=ax)\n",
    "        )\n",
    "        # prettify plot\n",
    "        if i < 7:\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "        ax.set_title('{} - {} obs'.format(weights, n))\n",
    "        \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power factor 1.5 seems to be nice. Powering even more will deplete the places with most observations very quickly. For the user this means that they first get all the well known destinations, and then the rest. The aim of our app is to surprise and inspire, so we also want to show more lesser known destinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV\n",
    "\n",
    "Add the sampling weight feature and write the final data set to be used by the frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_factor = 1.5\n",
    "\n",
    "output_df = (\n",
    "    df\n",
    "    # add the feature\n",
    "    .assign(weight = lambda df: (df['nr_tokens']**power_factor).astype(int))\n",
    "    # other hygiene\n",
    "    .drop(columns=['nr_tokens', 'ispartof', 'parentid'])\n",
    "    .set_index('id', drop=False)\n",
    "    # need to do this to convert numpy int and float to native data types\n",
    "    .astype('object')\n",
    ")\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write 'approved' file to the data and api folders\n",
    "output_df.to_csv(output_path1, index=False)\n",
    "output_df.to_csv(output_path2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
