{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Visual Crossing Weather API\n",
    "\n",
    "Goal of this notebook, see if we can get year round monthly aggregated historical weather data (temperature and precipitation) from the API given any geolocation. \n",
    "\n",
    "https://www.visualcrossing.com/weather-api.\n",
    "\n",
    "Pricing: 1000 free results per day. Or pro plan for 35 USD to just download the data. For these plans, will have to give attribution. See description of [pricing plans](https://www.visualcrossing.com/weather-data-editions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import logging \n",
    "\n",
    "load_dotenv()\n",
    "VISUALCROSSING_KEY = os.getenv(\"VISUALCROSSING_KEY\")\n",
    "\n",
    "S = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_dir = '../../api/data/'\n",
    "\n",
    "file_name = 'wikivoyage_destinations.csv'\n",
    "\n",
    "df_places = pd.read_csv(api_data_dir + file_name)#.set_index(\"id\", drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical summaries query\n",
    "\n",
    "This query can be used to fetch exactly what we want. Using the only API query editor we got the following query:\n",
    "\n",
    "```\n",
    "https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/historysummary?aggregateHours=24&combinationMethod=aggregate&maxStations=-1&maxDistance=-1&minYear=2000&maxYear=2020&chronoUnit=months&breakBy=self&dailySummaries=false&contentType=json&unitGroup=metric&locationMode=single&key=W8TURNQ78VLNPBK3MYPCMQDYS&dataElements=default&locations=25.7617%2C-80.1918\n",
    "```\n",
    "\n",
    "Let's translate that into a nice python call.\n",
    "\n",
    "See docs on [historical summaries api](https://www.visualcrossing.com/resources/documentation/weather-api/weather-api-documentation/) for more details.\n",
    "\n",
    "First compose a string with the geolocations of the places to query. It seems we get a time-out error if we query more than 4 destinations at once... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stairway.apis.visualcrossing.monthly_weather import get_visualcrossing_monthly_weather, await_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_locations = 2\n",
    "\n",
    "# add a column with comma seperated geolocation string\n",
    "df_places = df_places.assign(location = lambda df: \n",
    "                             df['lat'].round(6).astype(str) + \",\" + df['lng'].round(6).astype(str))\n",
    "\n",
    "def create_locations_string(df):\n",
    "    return '|'.join(df['location'].to_list())\n",
    "\n",
    "# crate pipe seperated geolocations string\n",
    "locations = create_locations_string(df_places.sample(n_locations))\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call the API. Note that the call takes quite some time to compute from their end. Therefore, submit calls [asynchronously](https://www.visualcrossing.com/resources/documentation/weather-api/how-to-submit-weather-api-asynchronously/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = get_visualcrossing_monthly_weather(locations, S, VISUALCROSSING_KEY)\n",
    "\n",
    "R.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = await_completion(R, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the result\n",
    "\n",
    "The results are in a nested json. This can easily be denormalized using the pandas `json_normalize()` function.\n",
    "\n",
    "Add the `name` and `tz` columns as additional metadata. `name` to join with the places dataframe, timezone for who knows what future purpose. Better save it if we are getting it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.io.json.json_normalize(data[\"locations\"], \"values\", [\"name\", \"tz\"])\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! \n",
    "\n",
    "After having queried all data, just join with the places dataframe to attach the stairway id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = (\n",
    "    pd.merge(df_places[['id', 'location']], df,  \n",
    "             how='inner', left_on=['location'], right_on = ['name'])\n",
    "    .drop(columns=['name', 'location'])\n",
    ")\n",
    "\n",
    "print(df_out.shape)\n",
    "\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented a script that does the above automatically in `scripts/visualcrossing_monthly_weather.py`.\n",
    "\n",
    "However, as we are only allowed to query 10 places at once and each query of 10 places takes around a minute, this would consume a lot of time to run for all places in our dataset. Therefore, try multithreading.\n",
    "\n",
    "*Note:* Having the threaded script working, there is no reason to keep this script as we can also set the number of threads to 1. This script is therefore archived (i.e. deleted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented in script `scripts/visualcrossing_monthly_weather_threaded.py`.\n",
    "\n",
    "This scripts sends the asynchronous API calls from multiple threads at the same time and waits for the data to come in. Each threat then parses the nested json data and appends it to a shared output csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing output\n",
    "\n",
    "We have gathered data with both the single threaded script and the multithreaded script. Also, the script wasn't perfect when we started querying the API, so we need to check which records are missing and try to see if we can fill in the blanks by running the query script once more. There might also be duplicate records due to script restarts on places that were already added.\n",
    "\n",
    "First combine both datasets after removing the duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('../../data/visualcrossing/visualcrossing_monthly_weather.csv').drop_duplicates()\n",
    "df_2 = (\n",
    "    pd.read_csv('../../data/visualcrossing/visualcrossing_monthly_weather_threaded.csv')\n",
    "    .drop_duplicates()\n",
    "    .drop('name', axis=1)  # Drop the name column only generated in the threaded script\n",
    ")\n",
    "\n",
    "df_out = pd.concat([df_1, df_2], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "print(df_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine how many of the dataset we got 12 records for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "collections.Counter(df_out['stairway_id'].value_counts().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again with all places that are missing or that have less than 12 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of places that we have succesfully queried\n",
    "successful_places = (\n",
    "    df_out\n",
    "    .groupby('stairway_id', as_index=False)\n",
    "    .agg(\n",
    "        count=pd.NamedAgg(column=\"stairway_id\", aggfunc=\"count\"),\n",
    "    )\n",
    "    .loc[lambda df: df['count'] == 12]  # successful means having 12 months\n",
    "    ['stairway_id']\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# subset places that we want to try again.\n",
    "df_retry = df_places.loc[~df_places['id'].isin(successful_places)]\n",
    "\n",
    "print('Successful:', len(successful_places))\n",
    "print('Yet to do:', len(df_retry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the retry dataset somewhere and rerun the multithreaded script on this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retry[['id', 'lat', 'lng']].to_csv('../../data/visualcrossing/wikivoyage_destinations_retry.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
