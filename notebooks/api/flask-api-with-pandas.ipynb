{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create queries for simple flask api with pandas\n",
    "\n",
    "Goal is to replace the call to the Firestore Database with a fetch from local disk.\n",
    "\n",
    "I.e. we will save a csv file on the app engine machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_dir = '../../api/data/'\n",
    "\n",
    "file_name = 'wikivoyage_destinations.csv'\n",
    "features_file_name = 'wikivoyage_features.csv'\n",
    "features_types = 'wikivoyage_features_types.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "\n",
    "Read data that has been prepared for the frontend by `feature_engineering.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(api_data_dir + file_name).set_index(\"id\", drop=False)\n",
    "df_features = pd.read_csv(api_data_dir + features_file_name).set_index(\"id\")\n",
    "df_feature_types = pd.read_csv(api_data_dir + features_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row lookup\n",
    "\n",
    "Read with index column on `pageid`, then use `.loc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[146019].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1).iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build queries\n",
    "\n",
    "### based on geo\n",
    "\n",
    "ne_lat, ne_lng, sw_lat, and sw_lng will be given:\n",
    "\n",
    "`&ne_lat=43.97363914475397&ne_lng=5.173845810128569&sw_lat=38.69043481932856&sw_lng=-0.5720037992464313`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_lat, ne_lng, sw_lat, sw_lng = 43.9, 5.17, 38.7, -0.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.resources.utils.selection import filter_on_geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .pipe(filter_on_geolocation, ne_lat, ne_lng, sw_lat, sw_lng)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling`\n",
    "\n",
    "What to do if the area is too small? Need to handle an error as there is no data available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_lat, ne_lng, sw_lat, sw_lng = 48.9, 2.47, 48.82, 2.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two cases: \n",
    "\n",
    "1. Really nothing can be found\n",
    "2. Only a small number can be found, for example less then the 10 requested by `sample(10)`\n",
    "\n",
    "Rather than sampling, let's try a technique where we sort the dataframe at random, and then pick the top x observations. This way, we can also work with offsets if we preserve the ordering. Preserve the ordering by setting a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .pipe(filter_on_geolocation, ne_lat, ne_lng, sw_lat, sw_lng)\n",
    ").sample(frac=1, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling\n",
    "\n",
    "Now, what is returned in case no records are found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_lat, ne_lng, sw_lat, sw_lng = 48.8, 2.2, 48.82, 2.22\n",
    "\n",
    "try:\n",
    "    (\n",
    "        df\n",
    "        .pipe(filter_on_geolocation, ne_lat, ne_lng, sw_lat, sw_lng)\n",
    "    ).sample(frac=1, random_state=1234)\n",
    "except ValueError:\n",
    "    print(\"Oops, ValueError! Must have at least one record. Return empty list?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offsets\n",
    "\n",
    "Select subset of results when working with an offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "offset = 0\n",
    "n_results = 3\n",
    "subset = df.sample(frac=1, random_state=1234).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.iloc[offset:offset+n_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted sampling\n",
    "\n",
    "In order to get some randomness, but still sample more important destinations first, use weights created in one of the feature engineering notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .sample(frac=1, random_state=1234, weights='weight')\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add top X destination features\n",
    "\n",
    "Given the place id, grab and sort the features directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.resources.utils.features import select_features, select_features_with_profiles, select_feature_columns_with_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_id = 662248\n",
    "top_x = 5\n",
    "\n",
    "select_features(dest_id, df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one should just have one feature\n",
    "select_features(146019, df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then simply add to the output doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df.loc[int(dest_id)].to_dict()\n",
    "\n",
    "doc[\"features\"] = select_features(doc['id'], df_features)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the explore endpoint, apply the function to all destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.sample(2)\n",
    "test['features'] = test['id'].apply(lambda x: select_features(x, df_features))\n",
    "test.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top features based on activity profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features from selected profiels first\n",
    "profiles = ['nature', 'active']\n",
    "select_features_with_profiles(dest_id, profiles, df_features, df_feature_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should show beaches\n",
    "select_features_with_profiles(197270, ['beach'], df_features, df_feature_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should return only one\n",
    "select_features_with_profiles(146019, ['beach'], df_features, df_feature_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearby places\n",
    "\n",
    "Given a place id, what are nearby place recommendations?\n",
    "\n",
    "Use Haverstine distance. Taken from this [stackoverflow](https://stackoverflow.com/questions/40452759/pandas-latitude-longitude-to-distance-between-successive-rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.resources.utils.distance import haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_id = 662248  # 's-Hertogenbosch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "#     .loc[lambda df: df['country'] == \"Netherlands\"]\n",
    "    .assign(lat_place = df.loc[dest_id]['lat'])\n",
    "    .assign(lng_place = df.loc[dest_id]['lng'])\n",
    "    .assign(distance = lambda x: haversine(x['lat'], x['lng'], x['lat_place'], x['lng_place']))\n",
    "    .sort_values('distance')\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to json\n",
    "\n",
    "Provide the places, as well as some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.resources.utils.utils import prettify_n_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of prettifying an X number of results for the front-end\n",
    "prettify_n_results(3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.sample(2).to_dict(orient='records')\n",
    "\n",
    "{\n",
    "    \"Results\": len(subset),\n",
    "    \"Results_string\": prettify_n_results(len(subset)),\n",
    "    \"Destinations\": subset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
