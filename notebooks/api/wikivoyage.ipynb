{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying wikivoyage\n",
    "\n",
    "In the frontend we want to include a small description of the place we are showing. If we use anything from wikipedia, we als need to give [attribution to the writers](https://en.wikivoyage.org/wiki/Wikivoyage:How_to_re-use_Wikivoyage_guides). This means we need to add a link to the wikipedia page where we got the text from.\n",
    "\n",
    "Check out the official API documentation [here](https://www.mediawiki.org/wiki/API:Tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "\n",
    "Below snippet is directly copied from the documentation and shows how to query using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    opensearch.py\n",
    "    MediaWiki API Demos\n",
    "    Demo of `Opensearch` module: Search the wiki and obtain\n",
    "\tresults in an OpenSearch (http://www.opensearch.org) format\n",
    "    MIT License\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"opensearch\",\n",
    "    \"namespace\": \"0\",\n",
    "    \"search\": \"Hampi\",\n",
    "    \"limit\": \"5\",\n",
    "    \"format\": \"json\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro text\n",
    "\n",
    "To fetch text, one has to use the `query/prop=extracts` parameter. A list of all the arguments for `extracts` is in the [official documentation](https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bextracts).\n",
    "\n",
    "How to retrieve only the intro text, this answer at [Stackoverflow](https://stackoverflow.com/questions/8555320/is-there-a-clean-wikipedia-api-just-for-retrieve-content-summary) suggests to use the `exintro` argument. The `redirects` can be used in combination with `titles` to find a location based on name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://en.wikivoyage.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"redirects\": 1,\n",
    "    \"titles\": \"Amsterdam\",\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"extracts\",  # yields extracts if available\n",
    "    \"exintro\": \"true\",\n",
    "    \"explaintext\": \"true\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed works pretty well!\n",
    "\n",
    "Now, we have the luxury that our database is based on wikivoyage. So why not query on pageid directly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"pageids\": 1036, # 1036 = amsterdam, 8966 Da Nang, 16935 Karachi\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"extracts\",  # yields extracts if available\n",
    "    \"exintro\": \"true\",\n",
    "    \"explaintext\": \"true\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problematic pages:\n",
    "* 8966 [Da Nang](https://en.wikivoyage.org/wiki/Da_Nang). This one doesn't return the intro text with `exintro`\n",
    "* 16935 Karachi. Has a very long intro. Maybe split on the first paragraph or apply `\\n\\n` formatting for alinea breaks? This can be solved in the frontend.\n",
    "\n",
    "To tackle the first problem, what can we do to get the intro extract?\n",
    "1. `query` wikivoyage on `title` -> but still no extract so doesn't work.\n",
    "2. `query` wikipedia on `pageid`? -> could be an extract there, but wikipedia has a lot more non-travel related info\n",
    "3. write own callback function to parse intro from html? -> get first sections using `exlimit=1` \n",
    "4. `opensearch` wikivoyage on name?\n",
    "5. showing only the first X sentences? -> but how many to pick?\n",
    "\n",
    "Let's explore option 3. Here you can see the different parameters for the [`query` format](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Bextracts). The idea is to use `exlimit=1` to get the first extract from wiki and then parse the intro section by splitting on section headers like `\"== Understand ==\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"pageids\": 8966, # 1036 = amsterdam, 8966 Da Nang, 16935 Karachi\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"extracts\",  # yields extracts if available\n",
    "#     \"exintro\": \"true\",\n",
    "    \"explaintext\": \"true\",\n",
    "    \"exlimit\": 1\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "\n",
    "data['query']['pages']['8966']['extract'].split('== Understand ==')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to website\n",
    "\n",
    "To give attribution, we need to add a small text like:\n",
    "\n",
    "\"A list of contributors is available at the original Singapore article at Wikivoyage.\"\n",
    "\n",
    "With a link to the contributors and the original article. Let's get the link to the article by using the [`info` prop](https://www.mediawiki.org/wiki/API:Info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"pageids\": 8966, # 1036 = amsterdam, 8966 Da Nang, 16935 Karachi\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": \"url\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "data = R.json()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['query']['pages']['8966']['fullurl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the title, the revision history page can be found by using the title `Da_Nang` in a url of the format: \n",
    "\n",
    "https://en.wikivoyage.org/w/index.php?title=Da_Nang&action=history.\n",
    "\n",
    "So basically, the `editurl` with another action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other info\n",
    "\n",
    "There's more stuff that can be queried using the API. For example, get a description of the place with:\n",
    "\n",
    "```\n",
    "     \"prop\": \"description\",  # yields 'description': 'municipality of Vietnam'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
